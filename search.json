[
  {
    "objectID": "notebooks/04_tools.html",
    "href": "notebooks/04_tools.html",
    "title": "Tools for file conversion and image processing",
    "section": "",
    "text": "Tools for file conversion and image processing\nHere we describe a series of tools that we have found useful for image Preprocessing, analysis and postprocessing\nSee our github.\nConverters."
  },
  {
    "objectID": "notebooks/01_microscopy.html",
    "href": "notebooks/01_microscopy.html",
    "title": "Introduction to Microscopy and Image Segmenation",
    "section": "",
    "text": "Recieve raw images (tiffs? convert to? alignment)\nSegment regions of interest\nAnalysis\nPublish\n\n\n\n\nImage Preprocessing:\n\nTechniques applied to raw microscopy images before segmentation, including normalization, denoising, and contrast enhancement, to improve the model’s ability to extract meaningful features.\n\nInstance Segmentation:\n\nIdentifying and delineating individual instances of objects within an image. In microscopy, this could involve distinguishing and outlining individual cells.\n\nSemantic Segmentation:\n\nAssigning a label to each pixel in an image, categorizing regions based on their semantic content. In microscopy, this could involve labeling different cellular structures or organelles.\n\nAnnotation/Labeling:\n\nThe process of manually marking and identifying specific features or objects of interest in an image dataset. In the context of image segmentation for microscopy, annotation involves creating pixel-level labels for regions like cells or organelles to train machine learning models.\n\nAI-assisted Labeling\n\nThe process of utilizing artificial intelligence (AI) algorithms to automate or enhance the manual annotation or labeling of data. In the context of image segmentation for microscopy, AI-assisted labeling involves the collaboration between human annotators and machine learning models. The algorithm assists human annotators by providing initial or suggested annotations, reducing the manual workload and potentially improving the efficiency and consistency of the labeling process. This symbiotic approach leverages the strengths of both human expertise and machine learning capabilities to accelerate the creation of labeled datasets for training segmentation models. AI-assisted labeling is particularly valuable in scenarios where large volumes of annotated data are required, and it helps bridge the gap between the capabilities of human annotators and the demands of training sophisticated machine learning models.\n\nTraining Set:\n\nThe subset of a dataset used to train a machine learning model. In image segmentation, this involves using annotated images to teach the model how to identify and delineate specific structures.\n\nTesting Set:\n\nThe subset of a dataset used to assess the performance and generalization of a trained model. In image segmentation, this involves evaluating how well the model can accurately segment unseen data.\n\nValidation Set:\n\nAn additional subset of the dataset used during the training phase to fine-tune model parameters and avoid overfitting. It helps ensure that the model generalizes well to new, unseen data.\n\nMasks:\n\nBinary images or pixel-level annotations that precisely define the boundaries of segmented objects in the original image. In the context of microscopy image segmentation, masks indicate the exact location and shape of identified structures, such as cells or subcellular components, facilitating quantitative analysis and interpretation.\n\nConvolutional Neural Network (CNN):\n\nA type of deep neural network designed for processing grid-structured data, such as images. CNNs are widely used in image segmentation tasks in microscopy.\n\nU-Net:\n\nA specific architecture of a convolutional neural network commonly used for biomedical image segmentation, including tasks in microscopy. Its distinctive U-shape allows for effective feature extraction and spatial resolution preservation.\n\nTransfer Learning:\n\nLeveraging pre-trained models on large datasets for related tasks to improve the performance of a model on a specific segmentation task in microscopy, where data may be limited.\n\nData Augmentation:\n\nTechniques to artificially increase the diversity of training data by applying transformations like rotation, flipping, or scaling. This is crucial for training robust segmentation models in microscopy.\n\nPost-processing:\n\nRefining segmentation results using techniques like morphological operations, contour smoothing, or merging to enhance the accuracy and coherence of the segmented regions."
  },
  {
    "objectID": "notebooks/01_microscopy.html#terms-of-reference",
    "href": "notebooks/01_microscopy.html#terms-of-reference",
    "title": "Introduction to Microscopy and Image Segmenation",
    "section": "",
    "text": "Image Preprocessing:\n\nTechniques applied to raw microscopy images before segmentation, including normalization, denoising, and contrast enhancement, to improve the model’s ability to extract meaningful features.\n\nInstance Segmentation:\n\nIdentifying and delineating individual instances of objects within an image. In microscopy, this could involve distinguishing and outlining individual cells.\n\nSemantic Segmentation:\n\nAssigning a label to each pixel in an image, categorizing regions based on their semantic content. In microscopy, this could involve labeling different cellular structures or organelles.\n\nAnnotation/Labeling:\n\nThe process of manually marking and identifying specific features or objects of interest in an image dataset. In the context of image segmentation for microscopy, annotation involves creating pixel-level labels for regions like cells or organelles to train machine learning models.\n\nAI-assisted Labeling\n\nThe process of utilizing artificial intelligence (AI) algorithms to automate or enhance the manual annotation or labeling of data. In the context of image segmentation for microscopy, AI-assisted labeling involves the collaboration between human annotators and machine learning models. The algorithm assists human annotators by providing initial or suggested annotations, reducing the manual workload and potentially improving the efficiency and consistency of the labeling process. This symbiotic approach leverages the strengths of both human expertise and machine learning capabilities to accelerate the creation of labeled datasets for training segmentation models. AI-assisted labeling is particularly valuable in scenarios where large volumes of annotated data are required, and it helps bridge the gap between the capabilities of human annotators and the demands of training sophisticated machine learning models.\n\nTraining Set:\n\nThe subset of a dataset used to train a machine learning model. In image segmentation, this involves using annotated images to teach the model how to identify and delineate specific structures.\n\nTesting Set:\n\nThe subset of a dataset used to assess the performance and generalization of a trained model. In image segmentation, this involves evaluating how well the model can accurately segment unseen data.\n\nValidation Set:\n\nAn additional subset of the dataset used during the training phase to fine-tune model parameters and avoid overfitting. It helps ensure that the model generalizes well to new, unseen data.\n\nMasks:\n\nBinary images or pixel-level annotations that precisely define the boundaries of segmented objects in the original image. In the context of microscopy image segmentation, masks indicate the exact location and shape of identified structures, such as cells or subcellular components, facilitating quantitative analysis and interpretation.\n\nConvolutional Neural Network (CNN):\n\nA type of deep neural network designed for processing grid-structured data, such as images. CNNs are widely used in image segmentation tasks in microscopy.\n\nU-Net:\n\nA specific architecture of a convolutional neural network commonly used for biomedical image segmentation, including tasks in microscopy. Its distinctive U-shape allows for effective feature extraction and spatial resolution preservation.\n\nTransfer Learning:\n\nLeveraging pre-trained models on large datasets for related tasks to improve the performance of a model on a specific segmentation task in microscopy, where data may be limited.\n\nData Augmentation:\n\nTechniques to artificially increase the diversity of training data by applying transformations like rotation, flipping, or scaling. This is crucial for training robust segmentation models in microscopy.\n\nPost-processing:\n\nRefining segmentation results using techniques like morphological operations, contour smoothing, or merging to enhance the accuracy and coherence of the segmented regions."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "We will be utilising Google Colab, you will need a google account to use this service."
  },
  {
    "objectID": "setup.html#optional.-singularity-access-on-artemis",
    "href": "setup.html#optional.-singularity-access-on-artemis",
    "title": "Setup",
    "section": "Optional. Singularity Access on Artemis",
    "text": "Optional. Singularity Access on Artemis\nIf you require the Singularity versions of AlphaFold, please log a request to be given access to Singularity."
  },
  {
    "objectID": "setup.html#a.-linux-systems",
    "href": "setup.html#a.-linux-systems",
    "title": "Setup",
    "section": "A. Linux systems",
    "text": "A. Linux systems\nIf you use Linux, then chances are you already have a shell. Open your preferred terminal program and off you go! An X-Window server (X11) may also be useful if you want to be able to use GUIs.\nConnection to Artemis can be made via ssh by issuing the following command on the shell:\nssh -X &lt;unikey&gt;@hpc.sydney.edu.au"
  },
  {
    "objectID": "setup.html#b.-osx-mac-computers-and-laptops",
    "href": "setup.html#b.-osx-mac-computers-and-laptops",
    "title": "Setup",
    "section": "B. OSX (Mac computers and laptops)",
    "text": "B. OSX (Mac computers and laptops)\nMac operating systems come with a terminal program, called Terminal. Just look for it in your Applications folder, or hit Command-Space and type ‘terminal’.\n\n\n\nTerminal is OSX’s native terminal emulator.\n\n\n\nWe also recommend installing XQuartz, which will replace OSX’s native X-Window server. XQuartz has some extra features that may offer better performance when using GUI programs. You’ll need to log out and back in again after installing XQuartz in order for it to activate.\nConnection to Artemis can be made via ssh by issuing the following command in the terminal:\nssh -X &lt;unikey&gt;@hpc.sydney.edu.au"
  },
  {
    "objectID": "setup.html#c.-windows",
    "href": "setup.html#c.-windows",
    "title": "Setup",
    "section": "C. Windows",
    "text": "C. Windows\nWindows has a couple of terminal programs and shells buried in the Programs menu (cmd, powershell). However, those aren’t going to work for us, as you’ll need extra programs and utilities to connect to Artemis, such as an SSH implementation. To use Artemis on Windows, you have a couple of options:\n\nOption i. PuTTY (Easy)\nPuTTY, an SSH and telnet client, is a good simple option.\nHead to the PuTTY Website and download PuTTY. You can install it to your computer, or just download the ‘binary’ and run it directly. Create a new session for use with Artemis as follows:\n\nFill in the connection details:\n\n\nHost Name: hpc.sydney.edu.au\nPort: 22\nConnection type: SSH\n\n\n\nName this session “Artemis” and click ‘Save’\n\nNote that PuTTY does not provide an X11 server natievly, so you won’t be able to use GUI programs on Artemis with just PuTTY. You can optionally install VcXsrv or similar to enable X-11 forwarding).\n\n\nOption ii. WSL and Ubuntu (Advanced)\nInstall Ubuntu or some other Linux distro on the Windows Subsystem for Linux see here for details. This one will give you the full suite of Linux functions."
  },
  {
    "objectID": "CHEATSHEET.html",
    "href": "CHEATSHEET.html",
    "title": "Quarto template cheatsheet",
    "section": "",
    "text": "Quarto template cheatsheet\nPlease contribute your tips, tricks for use, customisation of this template :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "",
    "text": "This is a training course aimed at users of the Sydney Microscopy and Microanalysis Core Research Facility at the University of Sydney who want to annotate their images using AI-assisted tools."
  },
  {
    "objectID": "index.html#watch-the-recording",
    "href": "index.html#watch-the-recording",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "Watch the Recording",
    "text": "Watch the Recording\nThis was delivered as an interactive workshop at Sydney. If you find yourself here in the future feel free to watch the recording:\nTODO"
  },
  {
    "objectID": "index.html#contributors",
    "href": "index.html#contributors",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "Contributors",
    "text": "Contributors\n\nSeb FMH\nNathaniel Butterworth SIH\nChad SIH\nAndre SIH"
  },
  {
    "objectID": "index.html#course-pre-requisites-and-setup-requirements",
    "href": "index.html#course-pre-requisites-and-setup-requirements",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "Course pre-requisites and setup requirements",
    "text": "Course pre-requisites and setup requirements\nNo previous programming experience is required, this training course will introduce you to fundamentals of the various tools (unix, Python) as required. Training will be delivered online, so you will need access to a modern computer with a stable internet connection. Participants will require the Sydney VPN and a terminal client (as per the Setup Instructions provided)."
  },
  {
    "objectID": "index.html#venue",
    "href": "index.html#venue",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "Venue",
    "text": "Venue\nOnline via Zoom, a link will be shared with registered participants.\n\nZoom etiquette and how we interact\nSessions will be recorded and added to this page. Please interrupt whenever you want! Ideally, have your camera on and interact as much as possible. There will be someone monitoring the chat-window for any questions you would like to post there. Extending for a three-hour duration, our Zoom session incorporates regular breaks and a blend of demonstrative and hands-on material."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nAs a University of Sydney course, we also want to make sure peopele are aware of our code of conduct. Feel free to move this to an about page as needed.\nExample standard Code of Conduct statement:\nWe expect all attendees of our training to follow our code of conduct, including bullying, harassment and discrimination prevention policies.\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOur full CoC, with incident reporting guidelines, is available Here."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nIn-depth\n\n\n\n\nAI Image Segmentaion Microscopy Context\n\n\n\nAI Assisted Labeling and Annotation\n\n\nOptional Topics\n\n\n\n\nAvizo Image Segmentation Workflow\n\n\n\nTools"
  },
  {
    "objectID": "index.html#setup-instructions",
    "href": "index.html#setup-instructions",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "Setup Instructions",
    "text": "Setup Instructions\nPlease attempt to complete the Setup Instructions before the course. We will run through this in the course too."
  },
  {
    "objectID": "notebooks/02_labeling.html",
    "href": "notebooks/02_labeling.html",
    "title": "Labeling Images",
    "section": "",
    "text": "After an image comes off a micrscope an expert may be able to identify all the different features in the image. IT still takes work to annotate these features. Several tools are availble that have different capabilities for marking up and labeling, or annotating, images. Traditional applications used in Microscopy include ImageJ/FIJI, Imod, Avizo/Amira.\nIn 2023 there are “AI-assisted” tools to accelerate the traditional labeling task. We have extensively tested many of these. At the time of writing some of these were in active and continual development, and some had been abandoned. We recommend three tools in particular and document their usage below.\n\n\nRecommended! Simple. Works anywhere. Tried on local Mac, Windows, Linux, with or without GPU.\n\n\n\nI installed this framework in a docker container locally (GTX 1650, 4GB VRAM). I could not get it to launch because of cuda out of memory issues, even with tiny pictures. I will try again on a bigger machine\nThese are the Docker instructions: https://github.com/Sydney-Informatics-Hub/micro-sam-contained\n\n\n\nAssisted annotations using MonAI has been done for “similar” workflows, but generally the pre-trained models seem a little too different to be beneficial for many microscopy tasks. The overhead for setting up a server is complex and the implementation is confusing. After annotating images, the resulting segmentation is very very poor, or simply crashes. As hinted at, learning the correct file types may be an issue, but also the models have just been trained to predict very specific image types. But MonAI seems worthy to keep in mind because it seems like it will be the most versatile and adaptable to other workflows (beyond this specific liver-cell segmentation). I have this working local with a NVIDIA GTX1650 4GB GPU, okay for testing, but need 16GB GPU for any training.\nGenerally, follow these instructions to get a working machine and then follow the demos below to start labeling.\n\n\nhttps://github.com/Project-MONAI/tutorials/blob/main/monailabel/monailabel_monaibundle_3dslicer_lung_nodule_detection.ipynb\n\n\n\nDoes not do a good job “out of the box”. Might need better training, but so far cannot get my labels to be used for training.\nhttps://github.com/Project-MONAI/tutorials/blob/main/monailabel/monailabel_pathology_HoVerNet_QuPath.ipynb\n\n\n\n\nhttps://github.com/obss/sahi\nhttps://github.com/matjesg/DeepFLaSH\nhttps://github.com/bingogome/samm\nhttps://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb\nhttps://github.com/roboflow/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb\nhttps://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb\nhttps://huggingface.co/spaces/IDEA-Research/Grounded-SAM"
  },
  {
    "objectID": "notebooks/02_labeling.html#anylabeling",
    "href": "notebooks/02_labeling.html#anylabeling",
    "title": "Labeling Images",
    "section": "",
    "text": "Recommended! Simple. Works anywhere. Tried on local Mac, Windows, Linux, with or without GPU."
  },
  {
    "objectID": "notebooks/02_labeling.html#segmentanything-for-microscopy",
    "href": "notebooks/02_labeling.html#segmentanything-for-microscopy",
    "title": "Labeling Images",
    "section": "",
    "text": "I installed this framework in a docker container locally (GTX 1650, 4GB VRAM). I could not get it to launch because of cuda out of memory issues, even with tiny pictures. I will try again on a bigger machine\nThese are the Docker instructions: https://github.com/Sydney-Informatics-Hub/micro-sam-contained"
  },
  {
    "objectID": "notebooks/02_labeling.html#monai",
    "href": "notebooks/02_labeling.html#monai",
    "title": "Labeling Images",
    "section": "",
    "text": "Assisted annotations using MonAI has been done for “similar” workflows, but generally the pre-trained models seem a little too different to be beneficial for many microscopy tasks. The overhead for setting up a server is complex and the implementation is confusing. After annotating images, the resulting segmentation is very very poor, or simply crashes. As hinted at, learning the correct file types may be an issue, but also the models have just been trained to predict very specific image types. But MonAI seems worthy to keep in mind because it seems like it will be the most versatile and adaptable to other workflows (beyond this specific liver-cell segmentation). I have this working local with a NVIDIA GTX1650 4GB GPU, okay for testing, but need 16GB GPU for any training.\nGenerally, follow these instructions to get a working machine and then follow the demos below to start labeling.\n\n\nhttps://github.com/Project-MONAI/tutorials/blob/main/monailabel/monailabel_monaibundle_3dslicer_lung_nodule_detection.ipynb\n\n\n\nDoes not do a good job “out of the box”. Might need better training, but so far cannot get my labels to be used for training.\nhttps://github.com/Project-MONAI/tutorials/blob/main/monailabel/monailabel_pathology_HoVerNet_QuPath.ipynb"
  },
  {
    "objectID": "notebooks/02_labeling.html#additional-image-segmentation-tools",
    "href": "notebooks/02_labeling.html#additional-image-segmentation-tools",
    "title": "Labeling Images",
    "section": "",
    "text": "https://github.com/obss/sahi\nhttps://github.com/matjesg/DeepFLaSH\nhttps://github.com/bingogome/samm\nhttps://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb\nhttps://github.com/roboflow/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb\nhttps://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb\nhttps://huggingface.co/spaces/IDEA-Research/Grounded-SAM"
  },
  {
    "objectID": "notebooks/03_avizo.html",
    "href": "notebooks/03_avizo.html",
    "title": "Image Segmentation with Avizo",
    "section": "",
    "text": "Image Segmentation with Avizo"
  }
]