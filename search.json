[
  {
    "objectID": "notebooks/02_labeling.html",
    "href": "notebooks/02_labeling.html",
    "title": "Labeling Images",
    "section": "",
    "text": "After an image comes off a micrscope an expert may be able to identify all the different features in the image. IT still takes work to annotate these features. Several tools are availble that have different capabilities for marking up and labeling, or annotating, images. Traditional applications used in Microscopy include ImageJ/FIJI, Imod, Avizo/Amira.\nIn 2023 there are “AI-assisted” tools to accelerate the traditional labeling task. We have extensively tested many of these. At the time of writing some of these were in active and continual development, and some had been abandoned. We recommend three tools in particular and document their usage below.\n\n\nRecommended! Simple. Works anywhere. Tried on local Mac, Windows, Linux, with or without GPU.\nFollow the documentation here. From this page https://github.com/vietanhdev/anylabeling/releases select the download to match your system.\n\n\n\nI installed this framework in a docker container locally (GTX 1650, 4GB VRAM). I could not get it to launch because of cuda out of memory issues, even with tiny pictures. I will try again on a bigger machine\nThese are the Docker instructions: https://github.com/Sydney-Informatics-Hub/micro-sam-contained\n\n\n\nAssisted annotations using MonAI has been done for “similar” workflows, but generally the pre-trained models seem a little too different to be beneficial for many microscopy tasks. The overhead for setting up a server is complex and the implementation is confusing. After annotating images, the resulting segmentation is very very poor, or simply crashes. As hinted at, learning the correct file types may be an issue, but also the models have just been trained to predict very specific image types. But MonAI seems worthy to keep in mind because it seems like it will be the most versatile and adaptable to other workflows (beyond this specific liver-cell segmentation). I have this working local with a NVIDIA GTX1650 4GB GPU, okay for testing, but need 16GB GPU for any training.\nGenerally, follow these instructions to get a working machine and then follow the demos below to start labeling.\n\n\nhttps://github.com/Project-MONAI/tutorials/blob/main/monailabel/monailabel_monaibundle_3dslicer_lung_nodule_detection.ipynb\n\n\n\nDoes not do a good job “out of the box”. Might need better training, but so far cannot get my labels to be used for training.\nhttps://github.com/Project-MONAI/tutorials/blob/main/monailabel/monailabel_pathology_HoVerNet_QuPath.ipynb\n\n\n\n\nhttps://github.com/obss/sahi\nhttps://github.com/matjesg/DeepFLaSH\nhttps://github.com/bingogome/samm\nhttps://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb\nhttps://github.com/roboflow/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb\nhttps://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb\nhttps://huggingface.co/spaces/IDEA-Research/Grounded-SAM"
  },
  {
    "objectID": "notebooks/02_labeling.html#anylabeling",
    "href": "notebooks/02_labeling.html#anylabeling",
    "title": "Labeling Images",
    "section": "",
    "text": "Recommended! Simple. Works anywhere. Tried on local Mac, Windows, Linux, with or without GPU.\nFollow the documentation here. From this page https://github.com/vietanhdev/anylabeling/releases select the download to match your system."
  },
  {
    "objectID": "notebooks/02_labeling.html#segmentanything-for-microscopy",
    "href": "notebooks/02_labeling.html#segmentanything-for-microscopy",
    "title": "Labeling Images",
    "section": "",
    "text": "I installed this framework in a docker container locally (GTX 1650, 4GB VRAM). I could not get it to launch because of cuda out of memory issues, even with tiny pictures. I will try again on a bigger machine\nThese are the Docker instructions: https://github.com/Sydney-Informatics-Hub/micro-sam-contained"
  },
  {
    "objectID": "notebooks/02_labeling.html#monai",
    "href": "notebooks/02_labeling.html#monai",
    "title": "Labeling Images",
    "section": "",
    "text": "Assisted annotations using MonAI has been done for “similar” workflows, but generally the pre-trained models seem a little too different to be beneficial for many microscopy tasks. The overhead for setting up a server is complex and the implementation is confusing. After annotating images, the resulting segmentation is very very poor, or simply crashes. As hinted at, learning the correct file types may be an issue, but also the models have just been trained to predict very specific image types. But MonAI seems worthy to keep in mind because it seems like it will be the most versatile and adaptable to other workflows (beyond this specific liver-cell segmentation). I have this working local with a NVIDIA GTX1650 4GB GPU, okay for testing, but need 16GB GPU for any training.\nGenerally, follow these instructions to get a working machine and then follow the demos below to start labeling.\n\n\nhttps://github.com/Project-MONAI/tutorials/blob/main/monailabel/monailabel_monaibundle_3dslicer_lung_nodule_detection.ipynb\n\n\n\nDoes not do a good job “out of the box”. Might need better training, but so far cannot get my labels to be used for training.\nhttps://github.com/Project-MONAI/tutorials/blob/main/monailabel/monailabel_pathology_HoVerNet_QuPath.ipynb"
  },
  {
    "objectID": "notebooks/02_labeling.html#additional-image-segmentation-tools",
    "href": "notebooks/02_labeling.html#additional-image-segmentation-tools",
    "title": "Labeling Images",
    "section": "",
    "text": "https://github.com/obss/sahi\nhttps://github.com/matjesg/DeepFLaSH\nhttps://github.com/bingogome/samm\nhttps://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb\nhttps://github.com/roboflow/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb\nhttps://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb\nhttps://huggingface.co/spaces/IDEA-Research/Grounded-SAM"
  },
  {
    "objectID": "notebooks/01_microscopy.html",
    "href": "notebooks/01_microscopy.html",
    "title": "Introduction to Microscopy and Image Segmenation",
    "section": "",
    "text": "Recieve raw images (tiffs? convert to? alignment)\nSegment regions of interest\nAnalysis\nPublish\n\n\n\n\nImage Preprocessing:\n\nTechniques applied to raw microscopy images before segmentation, including normalization, denoising, and contrast enhancement, to improve the model’s ability to extract meaningful features.\n\nInstance Segmentation:\n\nIdentifying and delineating individual instances of objects within an image. In microscopy, this could involve distinguishing and outlining individual cells.\n\nSemantic Segmentation:\n\nAssigning a label to each pixel in an image, categorizing regions based on their semantic content. In microscopy, this could involve labeling different cellular structures or organelles.\n\nAnnotation/Labeling:\n\nThe process of manually marking and identifying specific features or objects of interest in an image dataset. In the context of image segmentation for microscopy, annotation involves creating pixel-level labels for regions like cells or organelles to train machine learning models.\n\nAI-assisted Labeling\n\nThe process of utilizing artificial intelligence (AI) algorithms to automate or enhance the manual annotation or labeling of data. In the context of image segmentation for microscopy, AI-assisted labeling involves the collaboration between human annotators and machine learning models. The algorithm assists human annotators by providing initial or suggested annotations, reducing the manual workload and potentially improving the efficiency and consistency of the labeling process. This symbiotic approach leverages the strengths of both human expertise and machine learning capabilities to accelerate the creation of labeled datasets for training segmentation models. AI-assisted labeling is particularly valuable in scenarios where large volumes of annotated data are required, and it helps bridge the gap between the capabilities of human annotators and the demands of training sophisticated machine learning models.\n\nTraining Set:\n\nThe subset of a dataset used to train a machine learning model. In image segmentation, this involves using annotated images to teach the model how to identify and delineate specific structures.\n\nTesting Set:\n\nThe subset of a dataset used to assess the performance and generalization of a trained model. In image segmentation, this involves evaluating how well the model can accurately segment unseen data.\n\nValidation Set:\n\nAn additional subset of the dataset used during the training phase to fine-tune model parameters and avoid overfitting. It helps ensure that the model generalizes well to new, unseen data.\n\nMasks:\n\nBinary images or pixel-level annotations that precisely define the boundaries of segmented objects in the original image. In the context of microscopy image segmentation, masks indicate the exact location and shape of identified structures, such as cells or subcellular components, facilitating quantitative analysis and interpretation.\n\nConvolutional Neural Network (CNN):\n\nA type of deep neural network designed for processing grid-structured data, such as images. CNNs are widely used in image segmentation tasks in microscopy.\n\nU-Net:\n\nA specific architecture of a convolutional neural network commonly used for biomedical image segmentation, including tasks in microscopy. Its distinctive U-shape allows for effective feature extraction and spatial resolution preservation.\n\nTransfer Learning:\n\nLeveraging pre-trained models on large datasets for related tasks to improve the performance of a model on a specific segmentation task in microscopy, where data may be limited.\n\nData Augmentation:\n\nTechniques to artificially increase the diversity of training data by applying transformations like rotation, flipping, or scaling. This is crucial for training robust segmentation models in microscopy.\n\nPost-processing:\n\nRefining segmentation results using techniques like morphological operations, contour smoothing, or merging to enhance the accuracy and coherence of the segmented regions."
  },
  {
    "objectID": "notebooks/01_microscopy.html#terms-of-reference",
    "href": "notebooks/01_microscopy.html#terms-of-reference",
    "title": "Introduction to Microscopy and Image Segmenation",
    "section": "",
    "text": "Image Preprocessing:\n\nTechniques applied to raw microscopy images before segmentation, including normalization, denoising, and contrast enhancement, to improve the model’s ability to extract meaningful features.\n\nInstance Segmentation:\n\nIdentifying and delineating individual instances of objects within an image. In microscopy, this could involve distinguishing and outlining individual cells.\n\nSemantic Segmentation:\n\nAssigning a label to each pixel in an image, categorizing regions based on their semantic content. In microscopy, this could involve labeling different cellular structures or organelles.\n\nAnnotation/Labeling:\n\nThe process of manually marking and identifying specific features or objects of interest in an image dataset. In the context of image segmentation for microscopy, annotation involves creating pixel-level labels for regions like cells or organelles to train machine learning models.\n\nAI-assisted Labeling\n\nThe process of utilizing artificial intelligence (AI) algorithms to automate or enhance the manual annotation or labeling of data. In the context of image segmentation for microscopy, AI-assisted labeling involves the collaboration between human annotators and machine learning models. The algorithm assists human annotators by providing initial or suggested annotations, reducing the manual workload and potentially improving the efficiency and consistency of the labeling process. This symbiotic approach leverages the strengths of both human expertise and machine learning capabilities to accelerate the creation of labeled datasets for training segmentation models. AI-assisted labeling is particularly valuable in scenarios where large volumes of annotated data are required, and it helps bridge the gap between the capabilities of human annotators and the demands of training sophisticated machine learning models.\n\nTraining Set:\n\nThe subset of a dataset used to train a machine learning model. In image segmentation, this involves using annotated images to teach the model how to identify and delineate specific structures.\n\nTesting Set:\n\nThe subset of a dataset used to assess the performance and generalization of a trained model. In image segmentation, this involves evaluating how well the model can accurately segment unseen data.\n\nValidation Set:\n\nAn additional subset of the dataset used during the training phase to fine-tune model parameters and avoid overfitting. It helps ensure that the model generalizes well to new, unseen data.\n\nMasks:\n\nBinary images or pixel-level annotations that precisely define the boundaries of segmented objects in the original image. In the context of microscopy image segmentation, masks indicate the exact location and shape of identified structures, such as cells or subcellular components, facilitating quantitative analysis and interpretation.\n\nConvolutional Neural Network (CNN):\n\nA type of deep neural network designed for processing grid-structured data, such as images. CNNs are widely used in image segmentation tasks in microscopy.\n\nU-Net:\n\nA specific architecture of a convolutional neural network commonly used for biomedical image segmentation, including tasks in microscopy. Its distinctive U-shape allows for effective feature extraction and spatial resolution preservation.\n\nTransfer Learning:\n\nLeveraging pre-trained models on large datasets for related tasks to improve the performance of a model on a specific segmentation task in microscopy, where data may be limited.\n\nData Augmentation:\n\nTechniques to artificially increase the diversity of training data by applying transformations like rotation, flipping, or scaling. This is crucial for training robust segmentation models in microscopy.\n\nPost-processing:\n\nRefining segmentation results using techniques like morphological operations, contour smoothing, or merging to enhance the accuracy and coherence of the segmented regions."
  },
  {
    "objectID": "notebooks/03b_yolo.html",
    "href": "notebooks/03b_yolo.html",
    "title": "Image Segmentation with YOLO",
    "section": "",
    "text": "The YOLO computer vision model provides the tools and architecture for image segmentation tasks.\nIt is a command line/python tool, allows substantial flexibility, is open-source, and works on any platform.\nHere documented we use YoloV8 in its simplest form. Additional tools and advanced usage including YoloV5 can be found on the main repo\n\n\nStart an Anaconda prompt or similar and execute the following commands\nconda create -n yolo python=3.1\nconda activate yolo\npip install ultralytics==8.0.227\npip install scikit-learn==1.3.2\nThis will install all required packages.\n\n\n\nYOLO models need the data to be in a specific format and folder structure. Use the following tool to do this in one step assuming you created labels in [AnyLabeling]. If you created labels using some other method, check the [tools] folder for different conversion scripts.\npython tools/anylableing2yolo.py --class_labels '{\"mito\": 0, \"float\": 1}' --input_dir 'input_data' --output_dir 'datasets/mito_data' --split_ratio 0.2\nThis will create a new folder in datasets with the following structure:\nmito_data/\n├── images\n│   ├── train\n│   │   ├── image01.tif\n│   │   ├── image02.tif\n│   │   ├── image03.tif\n...\n│   ├── validate\n│   │   ├── image17.tif\n│   │   ├── image18.tif\n│   │   ├── image19.tif\n...\n├── labels\n│   ├── train\n│   │   ├── image01.txt\n│   │   ├── image02.txt\n│   │   ├── image03.txt\n...\n│   ├── validate\n│   │   ├── image17.tif\n│   │   ├── image18.tif\n│   │   ├── image19.tif\n\n\n\nNow you can execute a task that will use a pre-trained model as a starting point and use your data to refine the model specific to your prediction tasks.\nOn your command line execute the following\nyolo detect train data=mito.yaml model=yolov8m-seg.pt\nSee all the available command line options here https://docs.ultralytics.com/modes/train/#arguments\nThe choice of yolov8m-seg.pt is based on the performance https://docs.ultralytics.com/models/yolov8/#supported-tasks-and-modes and being specifically trained for semantic segmentation. Feel free to experiment with other models more tuned for your problem.\nThe structure of mito.yaml file is as follows:\npath: ../datasets/mito_data\ntrain: images/train  \nval: images/validate\ntest:  # test images (optional)\n\nnames:\n  0: mito\n  1: fat\nAdditional classes should be added (removed) as needed, and the paths should point to where your data are.\nIf all goes well this will produce the weights of a trained model in the default output folder (runs/segment/train/weights/best.pt).\nWe will now use this file to make predictions on new data!\n\n\n\nThe final step is to make predictions on new data\nyolo predict model=runs/segment/train/weights/best.pt source=path/to/raw_images/\nThis will output data. However, we need to customise the output using Python to get a clean image mask similar to Avizo output, and probably what most users want for microscopy. The prediction script is probably what we want.\npython predict.py model=runs/segment/train/weights/best.pt source=path/to/raw_images/\nWhere predict.py contains\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLOv8n-seg Segment model\nmodel = YOLO('runs/segment/train/weights/best.pt')\n\n# Run inference on an image\nresults = model(=path/to/raw_images/)  # results list\n\n# View results\nfor r in results:\n    print(r.masks)  # print the Masks object containing the detected instance masks"
  },
  {
    "objectID": "notebooks/03b_yolo.html#setup-your-python-environment",
    "href": "notebooks/03b_yolo.html#setup-your-python-environment",
    "title": "Image Segmentation with YOLO",
    "section": "",
    "text": "Start an Anaconda prompt or similar and execute the following commands\nconda create -n yolo python=3.1\nconda activate yolo\npip install ultralytics==8.0.227\npip install scikit-learn==1.3.2\nThis will install all required packages."
  },
  {
    "objectID": "notebooks/03b_yolo.html#labels-to-yolo-format",
    "href": "notebooks/03b_yolo.html#labels-to-yolo-format",
    "title": "Image Segmentation with YOLO",
    "section": "",
    "text": "YOLO models need the data to be in a specific format and folder structure. Use the following tool to do this in one step assuming you created labels in [AnyLabeling]. If you created labels using some other method, check the [tools] folder for different conversion scripts.\npython tools/anylableing2yolo.py --class_labels '{\"mito\": 0, \"float\": 1}' --input_dir 'input_data' --output_dir 'datasets/mito_data' --split_ratio 0.2\nThis will create a new folder in datasets with the following structure:\nmito_data/\n├── images\n│   ├── train\n│   │   ├── image01.tif\n│   │   ├── image02.tif\n│   │   ├── image03.tif\n...\n│   ├── validate\n│   │   ├── image17.tif\n│   │   ├── image18.tif\n│   │   ├── image19.tif\n...\n├── labels\n│   ├── train\n│   │   ├── image01.txt\n│   │   ├── image02.txt\n│   │   ├── image03.txt\n...\n│   ├── validate\n│   │   ├── image17.tif\n│   │   ├── image18.tif\n│   │   ├── image19.tif"
  },
  {
    "objectID": "notebooks/03b_yolo.html#training-your-model",
    "href": "notebooks/03b_yolo.html#training-your-model",
    "title": "Image Segmentation with YOLO",
    "section": "",
    "text": "Now you can execute a task that will use a pre-trained model as a starting point and use your data to refine the model specific to your prediction tasks.\nOn your command line execute the following\nyolo detect train data=mito.yaml model=yolov8m-seg.pt\nSee all the available command line options here https://docs.ultralytics.com/modes/train/#arguments\nThe choice of yolov8m-seg.pt is based on the performance https://docs.ultralytics.com/models/yolov8/#supported-tasks-and-modes and being specifically trained for semantic segmentation. Feel free to experiment with other models more tuned for your problem.\nThe structure of mito.yaml file is as follows:\npath: ../datasets/mito_data\ntrain: images/train  \nval: images/validate\ntest:  # test images (optional)\n\nnames:\n  0: mito\n  1: fat\nAdditional classes should be added (removed) as needed, and the paths should point to where your data are.\nIf all goes well this will produce the weights of a trained model in the default output folder (runs/segment/train/weights/best.pt).\nWe will now use this file to make predictions on new data!"
  },
  {
    "objectID": "notebooks/03b_yolo.html#inference",
    "href": "notebooks/03b_yolo.html#inference",
    "title": "Image Segmentation with YOLO",
    "section": "",
    "text": "The final step is to make predictions on new data\nyolo predict model=runs/segment/train/weights/best.pt source=path/to/raw_images/\nThis will output data. However, we need to customise the output using Python to get a clean image mask similar to Avizo output, and probably what most users want for microscopy. The prediction script is probably what we want.\npython predict.py model=runs/segment/train/weights/best.pt source=path/to/raw_images/\nWhere predict.py contains\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLOv8n-seg Segment model\nmodel = YOLO('runs/segment/train/weights/best.pt')\n\n# Run inference on an image\nresults = model(=path/to/raw_images/)  # results list\n\n# View results\nfor r in results:\n    print(r.masks)  # print the Masks object containing the detected instance masks"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "1. Get AnyLabeling\nDownload from:\n\n\n\n1. Get access to Avizo .\nContact:\n\n\n\n2. Get a Python installation\nGet Miniconda\n\n\n\n3. Other Tools\nOther tools are detailed throughout the documentation, you may need to install additional pieces of software depending on your workflow."
  },
  {
    "objectID": "CHEATSHEET.html",
    "href": "CHEATSHEET.html",
    "title": "Quarto template cheatsheet",
    "section": "",
    "text": "Quarto template cheatsheet\nPlease contribute your tips, tricks for use, customisation of this template :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "",
    "text": "This is a training course aimed at users of the Sydney Microscopy and Microanalysis Core Research Facility at the University of Sydney who want to annotate their images using AI-assisted tools."
  },
  {
    "objectID": "index.html#watch-the-recording",
    "href": "index.html#watch-the-recording",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "Watch the Recording",
    "text": "Watch the Recording\nThis was delivered as an interactive workshop at Sydney. If you find yourself here in the future feel free to watch the recording:\nTODO"
  },
  {
    "objectID": "index.html#contributors",
    "href": "index.html#contributors",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "Contributors",
    "text": "Contributors\n\nSeb SIH\nNathaniel Butterworth SIH\nChad Moore SMM\nAndre Venne SMM"
  },
  {
    "objectID": "index.html#course-pre-requisites-and-setup-requirements",
    "href": "index.html#course-pre-requisites-and-setup-requirements",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "Course pre-requisites and setup requirements",
    "text": "Course pre-requisites and setup requirements\nNo previous programming experience is required, this training course will introduce you to fundamentals of the various tools (unix, Python) as required. Training will be delivered online, so you will need access to a modern computer with a stable internet connection. Participants will require the Sydney VPN and a terminal client (as per the Setup Instructions provided)."
  },
  {
    "objectID": "index.html#venue",
    "href": "index.html#venue",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "Venue",
    "text": "Venue\nOnline via Zoom, a link will be shared with registered participants.\n\nZoom etiquette and how we interact\nSessions will be recorded and added to this page. Please interrupt whenever you want! Ideally, have your camera on and interact as much as possible. There will be someone monitoring the chat-window for any questions you would like to post there. Extending for a three-hour duration, our Zoom session incorporates regular breaks and a blend of demonstrative and hands-on material."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nAs a University of Sydney course, we also want to make sure peopele are aware of our code of conduct. Feel free to move this to an about page as needed.\nExample standard Code of Conduct statement:\nWe expect all attendees of our training to follow our code of conduct, including bullying, harassment and discrimination prevention policies.\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOur full CoC, with incident reporting guidelines, is available Here."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nIn-depth\n\n\n\n\nAI Image Segmentaion Microscopy Context\n\n\n\nAI Assisted Labeling and Annotation\n\n\n\nAvizo Image Segmentation Workflow\n\n\n\nYolo Image Segmentation Workflow\n\n\n\nConversion and Data Analysis Tools"
  },
  {
    "objectID": "index.html#setup-instructions",
    "href": "index.html#setup-instructions",
    "title": "Image Segmentation at Sydney Microscopy and Microanalysis",
    "section": "Setup Instructions",
    "text": "Setup Instructions\nPlease attempt to complete the Setup Instructions before the course. We will run through this in the course too."
  },
  {
    "objectID": "notebooks/04_tools.html",
    "href": "notebooks/04_tools.html",
    "title": "Tools for file conversion and image processing",
    "section": "",
    "text": "Tools for file conversion and image processing\nHere we describe a series of tools that we have found useful for image Preprocessing, analysis and postprocessing\nSee our toolkit on the Microscopy-Segmentation github.\nConverters."
  },
  {
    "objectID": "notebooks/03_avizo.html",
    "href": "notebooks/03_avizo.html",
    "title": "Image Segmentation with Avizo",
    "section": "",
    "text": "Image Segmentation with Avizo\nAmira/Avizo3D is a commercial 3D image data visualisation and analysis toolkit. It only runs on Windows, and for Deep Learning (re: image segmentation) task it requires a reasonably specced GPU.\nIf you have access to an appropriate machine with a license, follow the deck below to complete a full image segmentation work flow."
  }
]